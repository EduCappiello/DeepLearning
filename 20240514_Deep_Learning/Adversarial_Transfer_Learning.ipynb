{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Discriminator Loss: 1.4074294567108154, Generator Loss: 0.6290392279624939, Task Network Loss: 1.0926198959350586\n",
      "Epoch [10/100], Discriminator Loss: 1.3915977478027344, Generator Loss: 0.6163039207458496, Task Network Loss: 1.0637712478637695\n",
      "Epoch [20/100], Discriminator Loss: 1.367431879043579, Generator Loss: 0.6166011691093445, Task Network Loss: 1.0435699224472046\n",
      "Epoch [30/100], Discriminator Loss: 1.341942548751831, Generator Loss: 0.6241279244422913, Task Network Loss: 1.0181152820587158\n",
      "Epoch [40/100], Discriminator Loss: 1.3245271444320679, Generator Loss: 0.63446444272995, Task Network Loss: 0.9937199950218201\n",
      "Epoch [50/100], Discriminator Loss: 1.309558629989624, Generator Loss: 0.6493899822235107, Task Network Loss: 0.9652581810951233\n",
      "Epoch [60/100], Discriminator Loss: 1.2380821704864502, Generator Loss: 0.7220100164413452, Task Network Loss: 0.926879346370697\n",
      "Epoch [70/100], Discriminator Loss: 1.168175458908081, Generator Loss: 0.7951335906982422, Task Network Loss: 0.8698365092277527\n",
      "Epoch [80/100], Discriminator Loss: 1.1350500583648682, Generator Loss: 0.8122456073760986, Task Network Loss: 0.7921953797340393\n",
      "Epoch [90/100], Discriminator Loss: 1.1136802434921265, Generator Loss: 0.7972151637077332, Task Network Loss: 0.7091919183731079\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into source and target domains\n",
    "X_source, X_target, y_source, y_target = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_source = torch.from_numpy(X_source).float()\n",
    "X_target = torch.from_numpy(X_target).float()\n",
    "y_source = torch.from_numpy(y_source).long()\n",
    "\n",
    "# Define the feature extractor\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Adjusted input_size to match the number of features\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)  # Change input_size to match the output size of the feature extractor\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "# Define the task-specific network\n",
    "class TaskNetwork(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(TaskNetwork, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(input_size)\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, 3)  # Output layer for Iris dataset with 3 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Initialize the models\n",
    "feature_extractor = FeatureExtractor(input_size=4)\n",
    "# Pass the output size of the feature extractor to the discriminator\n",
    "discriminator = Discriminator(input_size=32)\n",
    "generator = Generator()\n",
    "task_network = TaskNetwork(input_size=4)\n",
    "\n",
    "\n",
    "# Define the adversarial learning process\n",
    "class AdversarialNetwork(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(AdversarialNetwork, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "    \n",
    "    def forward(self, x):\n",
    "        generated = self.generator(x)\n",
    "        return self.discriminator(generated)\n",
    "\n",
    "# Initialize the adversarial network\n",
    "adversarial_network = AdversarialNetwork(generator, discriminator)\n",
    "\n",
    "# Define optimizers\n",
    "optimizer_gen = optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_disc = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "optimizer_task = optim.Adam(task_network.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    optimizer_disc.zero_grad()\n",
    "    real_target_output = discriminator(feature_extractor(X_target))  # Apply feature extractor to target data\n",
    "    fake_output = discriminator(generator(X_source))\n",
    "    loss_discriminator = F.binary_cross_entropy(fake_output, torch.zeros_like(fake_output)) + \\\n",
    "                        F.binary_cross_entropy(real_target_output, torch.ones_like(real_target_output))\n",
    "    loss_discriminator.backward()\n",
    "    optimizer_disc.step()\n",
    "\n",
    "    # Train generator\n",
    "    optimizer_gen.zero_grad()\n",
    "    fake_output = discriminator(generator(X_source))\n",
    "    loss_generator = F.binary_cross_entropy(fake_output, torch.ones_like(fake_output))\n",
    "    loss_generator.backward()\n",
    "    optimizer_gen.step()\n",
    "\n",
    "    # Train task network\n",
    "    optimizer_task.zero_grad()\n",
    "    source_features = feature_extractor(X_source)\n",
    "    predicted = task_network(X_source)\n",
    "    loss_task = F.cross_entropy(predicted, y_source)\n",
    "    loss_task.backward()\n",
    "    optimizer_task.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Discriminator Loss: {loss_discriminator.item()}, '\n",
    "              f'Generator Loss: {loss_generator.item()}, Task Network Loss: {loss_task.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(X_test, y_test):\n",
    "    # Make predictions\n",
    "    task_network.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        y_pred_probs = task_network(X_test)\n",
    "        _, y_pred = torch.max(y_pred_probs, 1)  # Convert probabilities to class labels\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
    "    precision = precision_score(y_test.numpy(), y_pred.numpy(), average='macro')\n",
    "    recall = recall_score(y_test.numpy(), y_pred.numpy(), average='macro')\n",
    "    f1 = f1_score(y_test.numpy(), y_pred.numpy(), average='macro')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test.numpy(), y_pred.numpy())\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Splitting the test data\n",
    "X_source_test, X_target_test, y_source_test, y_target_test = train_test_split(X_target, y_target, test_size=0.5, random_state=42)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, cm = evaluate_model(X_source_test, torch.from_numpy(y_source_test).long())  # Here, we convert y_source_test to PyTorch tensor\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
