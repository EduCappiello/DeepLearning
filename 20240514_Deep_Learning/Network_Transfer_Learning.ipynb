{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], Task Network Loss: 1.1001805067062378\n",
      "Epoch [10/500], Task Network Loss: 1.0939676761627197\n",
      "Epoch [20/500], Task Network Loss: 1.0903115272521973\n",
      "Epoch [30/500], Task Network Loss: 1.087016224861145\n",
      "Epoch [40/500], Task Network Loss: 1.0831530094146729\n",
      "Epoch [50/500], Task Network Loss: 1.078602910041809\n",
      "Epoch [60/500], Task Network Loss: 1.073197841644287\n",
      "Epoch [70/500], Task Network Loss: 1.0667837858200073\n",
      "Epoch [80/500], Task Network Loss: 1.0591986179351807\n",
      "Epoch [90/500], Task Network Loss: 1.0502936840057373\n",
      "Epoch [100/500], Task Network Loss: 1.0399631261825562\n",
      "Epoch [110/500], Task Network Loss: 1.0281555652618408\n",
      "Epoch [120/500], Task Network Loss: 1.0149377584457397\n",
      "Epoch [130/500], Task Network Loss: 1.0005426406860352\n",
      "Epoch [140/500], Task Network Loss: 0.985356867313385\n",
      "Epoch [150/500], Task Network Loss: 0.9699095487594604\n",
      "Epoch [160/500], Task Network Loss: 0.9547646045684814\n",
      "Epoch [170/500], Task Network Loss: 0.9404261112213135\n",
      "Epoch [180/500], Task Network Loss: 0.9272337555885315\n",
      "Epoch [190/500], Task Network Loss: 0.9152957201004028\n",
      "Epoch [200/500], Task Network Loss: 0.9045194387435913\n",
      "Epoch [210/500], Task Network Loss: 0.8947411179542542\n",
      "Epoch [220/500], Task Network Loss: 0.8857389092445374\n",
      "Epoch [230/500], Task Network Loss: 0.8773545622825623\n",
      "Epoch [240/500], Task Network Loss: 0.8694874048233032\n",
      "Epoch [250/500], Task Network Loss: 0.8620343208312988\n",
      "Epoch [260/500], Task Network Loss: 0.8548657298088074\n",
      "Epoch [270/500], Task Network Loss: 0.8479514718055725\n",
      "Epoch [280/500], Task Network Loss: 0.8412233591079712\n",
      "Epoch [290/500], Task Network Loss: 0.8346440196037292\n",
      "Epoch [300/500], Task Network Loss: 0.8281948566436768\n",
      "Epoch [310/500], Task Network Loss: 0.8218406438827515\n",
      "Epoch [320/500], Task Network Loss: 0.8155607581138611\n",
      "Epoch [330/500], Task Network Loss: 0.8093410730361938\n",
      "Epoch [340/500], Task Network Loss: 0.803174614906311\n",
      "Epoch [350/500], Task Network Loss: 0.7970479130744934\n",
      "Epoch [360/500], Task Network Loss: 0.7909591197967529\n",
      "Epoch [370/500], Task Network Loss: 0.7849385142326355\n",
      "Epoch [380/500], Task Network Loss: 0.7789744734764099\n",
      "Epoch [390/500], Task Network Loss: 0.7730669975280762\n",
      "Epoch [400/500], Task Network Loss: 0.7672221660614014\n",
      "Epoch [410/500], Task Network Loss: 0.7614457607269287\n",
      "Epoch [420/500], Task Network Loss: 0.7557518482208252\n",
      "Epoch [430/500], Task Network Loss: 0.7501548528671265\n",
      "Epoch [440/500], Task Network Loss: 0.7446650266647339\n",
      "Epoch [450/500], Task Network Loss: 0.73929762840271\n",
      "Epoch [460/500], Task Network Loss: 0.7340502142906189\n",
      "Epoch [470/500], Task Network Loss: 0.7289313673973083\n",
      "Epoch [480/500], Task Network Loss: 0.7239468097686768\n",
      "Epoch [490/500], Task Network Loss: 0.7191015481948853\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into source and target domains\n",
    "X_source, X_target, y_source, y_target = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_source = torch.from_numpy(X_source).float()\n",
    "X_target = torch.from_numpy(X_target).float()\n",
    "y_source = torch.from_numpy(y_source).long()\n",
    "\n",
    "# Define the feature extractor\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the task-specific network\n",
    "class TaskNetwork(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(TaskNetwork, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(input_size)\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, 3)  # Output layer for Iris dataset with 3 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Initialize the models\n",
    "task_network = TaskNetwork(input_size=4)\n",
    "\n",
    "# Freeze the parameters of the pre-trained feature extractor\n",
    "for param in task_network.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define optimizer for the task-specific network only\n",
    "optimizer_task = optim.Adam(task_network.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    # Train task network\n",
    "    task_network.train()  # Set the task network to training mode\n",
    "    optimizer_task.zero_grad()\n",
    "    source_features = task_network.feature_extractor(X_source)\n",
    "    predicted = task_network(X_source)\n",
    "    loss_task = F.cross_entropy(predicted, y_source)\n",
    "    loss_task.backward()\n",
    "    optimizer_task.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Task Network Loss: {loss_task.item()}')\n",
    "\n",
    "# Evaluate the model (similar to previous evaluation code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(X_test, y_test, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        y_pred_probs = model(X_test)\n",
    "        _, y_pred = torch.max(y_pred_probs, 1)  # Convert probabilities to class labels\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
    "    precision = precision_score(y_test.numpy(), y_pred.numpy(), average='macro')\n",
    "    recall = recall_score(y_test.numpy(), y_pred.numpy(), average='macro')\n",
    "    f1 = f1_score(y_test.numpy(), y_pred.numpy(), average='macro')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test.numpy(), y_pred.numpy())\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Splitting the test data\n",
    "X_source_test, X_target_test, y_source_test, y_target_test = train_test_split(X_target, y_target, test_size=0.5, random_state=42)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, cm = evaluate_model(X_source_test, torch.from_numpy(y_source_test).long(), task_network)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
